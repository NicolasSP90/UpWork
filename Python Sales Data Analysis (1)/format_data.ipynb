{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p><a href=\"https://www.upwork.com/jobs/Python-Sales-Data-Analysis_~01206da2beb9fe65db/?referrer_url_path=find_work_home\">Upwork Link</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0YKtvWQ8-1s"
      },
      "source": [
        "This script formats raw sales data from Toast, Square, and Clover (to be\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "implemented) into data adhering to the Flapjack data format.\n",
        "\n",
        "**The Flapjack data format has the following columns:**\n",
        "\n",
        "- `date`: Date of the order\n",
        "\n",
        "- `item`: Name of the item ordered\n",
        "\n",
        "- `price`: Price of the item\n",
        "\n",
        "- `order_id`: Unique ID of the order this item is associated with\n",
        "\n",
        "Each row is associated with a line item in an order. If there are multiple of the same item in a single order (i.e. two cheeseburgers in one order), there are two separate but identical rows for cheeseburgers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMFtqvAe85wH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmfMfjDc_Esi"
      },
      "outputs": [],
      "source": [
        "def convert_dollars_to_float(df, columnName):\n",
        "    df[columnName] = df[columnName].replace('[\\$,]', '', regex=True).astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sO2R5Wb3_IIS"
      },
      "outputs": [],
      "source": [
        "'''Expects data of the format { date, item, qty, price, order_id }'''\n",
        "def standardize_quantity(df):\n",
        "    df_copy = df.copy()\n",
        "    df_copy['to_delete'] = False\n",
        "\n",
        "    rows_to_append = []\n",
        "    for index, row in df_copy.iterrows():\n",
        "        if row['qty'] > 1:\n",
        "            df_copy.at[index, 'to_delete'] = True\n",
        "            for i in range(row['qty']):\n",
        "                new_row = {\n",
        "                    'date': row['date'],\n",
        "                    'item': row['item'],\n",
        "                    'price': row['price'] / row['qty'],\n",
        "                    'order_id': row['order_id'],\n",
        "                }\n",
        "                rows_to_append.append(new_row)\n",
        "    df_copy = df_copy[df_copy['to_delete'] == False]\n",
        "    df_copy.drop(columns=['to_delete', 'qty'], inplace=True)\n",
        "\n",
        "    new_df = pd.DataFrame(rows_to_append)\n",
        "    df = pd.concat([df, new_df])\n",
        "\n",
        "    return df_copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTUV5ORmID2-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def convert_dollars_to_float(df, columnName):\n",
        "  df[columnName] = df[columnName].replace('[\\$,]', '', regex=True).astype(float)\n",
        "\n",
        "'''Expects data of the format { date, item, qty, price, order_id }'''\n",
        "def standardize_quantity(df):\n",
        "    df_copy = df.copy()\n",
        "    df_copy['to_delete'] = False\n",
        "\n",
        "    rows_to_append = []\n",
        "    for index, row in df_copy.iterrows():\n",
        "        if row['qty'] > 1:\n",
        "            df_copy.at[index, 'to_delete'] = True\n",
        "            for i in range(row['qty']):\n",
        "                new_row = {\n",
        "                    'date': row['date'],\n",
        "                    'item': row['item'],\n",
        "                    'price': row['price'] / row['qty'],\n",
        "                    'order_id': row['order_id'],\n",
        "                }\n",
        "                rows_to_append.append(new_row)\n",
        "    df_copy = df_copy[df_copy['to_delete'] == False]\n",
        "    df_copy.drop(columns=['to_delete', 'qty'], inplace=True)\n",
        "\n",
        "    new_df = pd.DataFrame(rows_to_append)\n",
        "    df = pd.concat([df, new_df])\n",
        "\n",
        "    return df_copy\n",
        "\n",
        "def format_square_data(csv_path):\n",
        "    columns_to_include = ['Date', 'Item', 'Qty', 'Net Sales', 'Transaction ID']\n",
        "    df = pd.read_csv(csv_path, usecols=columns_to_include, parse_dates=['Date'])\n",
        "\n",
        "    renamed_columns = {\n",
        "        'Date': 'date',\n",
        "        'Item': 'item',\n",
        "        'Qty': 'qty',\n",
        "        'Net Sales': 'price',\n",
        "        'Transaction ID': 'order_id',\n",
        "    }\n",
        "    df.rename(columns=renamed_columns, inplace=True)\n",
        "\n",
        "    convert_dollars_to_float(df, 'price')\n",
        "\n",
        "    # Get rid of negative quantities\n",
        "    df = df[df['qty'] >= 1]\n",
        "\n",
        "    # Turn multi-item line items into single-item\n",
        "    df = standardize_quantity(df)\n",
        "\n",
        "    return df\n",
        "\n",
        "def format_clover_data(csv_path):\n",
        "    pass\n",
        "\n",
        "def format_toast_data(csv_path):\n",
        "    columns_to_include = ['Order Date', 'Menu Item', 'Qty', 'Net Price', 'Order Id']\n",
        "    df = pd.read_csv(csv_path, usecols=columns_to_include, parse_dates=['Order Date'])\n",
        "\n",
        "    renamed_columns = {\n",
        "        'Order Date': 'date',\n",
        "        'Menu Item': 'item',\n",
        "        'Qty': 'qty',\n",
        "        'Net Price': 'price',\n",
        "        'Order Id': 'order_id',\n",
        "    }\n",
        "    df.rename(columns=renamed_columns, inplace=True)\n",
        "    df['order_id'] = df['order_id'].apply(str)\n",
        "\n",
        "    df['qty'] = df['qty'].astype(int)\n",
        "\n",
        "    # Get rid of fractional quantities\n",
        "    df = df[df['qty'] >= 1]\n",
        "\n",
        "    # Turn multi-item line items into single-item\n",
        "    df = standardize_quantity(df)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ks3XRFRbjPZd"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Og9B_K30jXam"
      },
      "outputs": [],
      "source": [
        "# Adjust the paths to point to the correct location in your Google Drive\n",
        "file_path_square = '/content/drive/My Drive/items-2023-11-01-2023-12-01.csv'\n",
        "file_path_toast = '/content/drive/My Drive/ItemSelectionDetails_2023_11_14-2023_12_13.csv'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_iC_rUekkD3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read and display the first few rows of each file to understand their structure\n",
        "square_df = pd.read_csv(file_path_square)\n",
        "try:\n",
        "    toast_df = pd.read_csv(file_path_toast, encoding='ISO-8859-1')\n",
        "except Exception as e:\n",
        "    read_error = e\n",
        "else:\n",
        "    read_error = None\n",
        "\n",
        "(toast_df.head() if read_error is None else read_error), square_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_j9Rt6igWId"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, time\n",
        "\n",
        "def set_square_service_times(df, service_times):\n",
        "    \"\"\"\n",
        "    Categorize Square data records based on user-defined service times.\n",
        "\n",
        "    :param df: DataFrame containing Square POS data.\n",
        "    :param service_times: Dictionary defining service times, e.g., {'Breakfast': ('08:00', '12:00'), 'Dinner': ('12:00', '24:00')}\n",
        "    :return: DataFrame with an additional 'Service' column categorizing each record.\n",
        "    \"\"\"\n",
        "    # Function to determine the service based on a given time\n",
        "    def determine_service(record_time):\n",
        "        for service, (start, end) in service_times.items():\n",
        "            if start <= record_time < end:\n",
        "                return service\n",
        "        return \"Undefined\"\n",
        "\n",
        "    # Convert 'Time' column to Python time objects for comparison\n",
        "    df['Time'] = pd.to_datetime(df['Time'], format='%H:%M:%S').dt.time\n",
        "\n",
        "    # Apply the service categorization\n",
        "    df['Service'] = df['Time'].apply(determine_service)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPTaqq1fqfip"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "example_service_times = {\n",
        "    'Breakfast': (time(8, 0), time(12, 0)),\n",
        "    'Dinner': (time(12, 0), time(23, 59))\n",
        "}\n",
        "\n",
        "# Apply thhe Squae function to tre data with example service times\n",
        "square_df_service_categorized = set_square_service_times(square_df.copy(), example_service_times)\n",
        "square_df_service_categorized.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZRRkpx0lwlY"
      },
      "source": [
        "# **How to Use the Function:**\n",
        "Define your service times in a dictionary, specifying the start and end times for each service.\n",
        "Call set_square_service_times with your Square data and the service times dictionary.\n",
        "Example:\n",
        "```\n",
        "service_times = {\n",
        "    'Breakfast': (time(8, 0), time(12, 0)),\n",
        "    'Lunch': (time(12, 0), time(17, 0)),\n",
        "    'Dinner': (time(17, 0), time(23, 59))\n",
        "}\n",
        "\n",
        "categorized_data = set_square_service_times(square_data, service_times)\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt6BkyrkmNFU"
      },
      "source": [
        "# **Task 2 - Convert Data to Standard Format**\n",
        "Now, we'll proceed with converting both Toast POS and Square POS data to the standard Flapjack format. This format includes the following columns: date, item, price, and order_id."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_LPi8zrlqXr"
      },
      "outputs": [],
      "source": [
        "# Converting Toast POS Data to Standard Format\n",
        "\n",
        "# Selecting and renaming the relevant columns for Toast data\n",
        "toast_columns_mapping = {\n",
        "    'Order Date': 'date',\n",
        "    'Menu Item': 'item',\n",
        "    'Net Price': 'price',\n",
        "    'Order Id': 'order_id'\n",
        "}\n",
        "toast_standard_df = toast_df[toast_columns_mapping.keys()].rename(columns=toast_columns_mapping)\n",
        "\n",
        "# Convert price from string to float and order_id to string\n",
        "toast_standard_df['price'] = toast_standard_df['price'].replace('[\\$,]', '', regex=True).astype(float)\n",
        "toast_standard_df['order_id'] = toast_standard_df['order_id'].astype(str)\n",
        "\n",
        "# Displaying a few rows of the standardized Toast data\n",
        "toast_standard_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtD4WDtDmVdI"
      },
      "source": [
        "# **Converting Square POS Data to Standard Format**\n",
        "Next, I'll convert the Square POS data to the Flapjack format. Since we've already categorized the data by service times in Task 1, we'll include that in the standardized format as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4vmab5dmJyh"
      },
      "outputs": [],
      "source": [
        "# Converting Square POS Data to Standard Format\n",
        "\n",
        "# Selecting and renaming the relevant columns for Square data\n",
        "square_columns_mapping = {\n",
        "    'Date': 'date',\n",
        "    'Item': 'item',\n",
        "    'Gross Sales': 'price',\n",
        "    'Service': 'order_id'  # Using the 'Service' column as a placeholder for 'order_id'\n",
        "}\n",
        "square_standard_df = square_df_service_categorized[square_columns_mapping.keys()].rename(columns=square_columns_mapping)\n",
        "\n",
        "# Convert price from string to float\n",
        "square_standard_df['price'] = square_standard_df['price'].replace('[\\$,]', '', regex=True).astype(float)\n",
        "\n",
        "# Displaying a few rows of the standardized Square data\n",
        "square_standard_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U34YzY7hnTvQ"
      },
      "source": [
        "# **Task 3 - Top Selling Times/Services**\n",
        "In this task, we will analyze the sales data to uncover insights into:\n",
        "\n",
        "Gross Sales Throughout the Day (Grouped by Hour): Identifying peak sales times.\n",
        "Revenue by Service: Determining which service generates the most sales.\n",
        "Revenue Throughout the Week: Analyzing which day of the week records the most sales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDTbSr0GnGgN"
      },
      "outputs": [],
      "source": [
        "# Converting 'date' columns to datetime for both datasets\n",
        "toast_standard_df['date'] = pd.to_datetime(toast_standard_df['date'])\n",
        "square_standard_df['date'] = pd.to_datetime(square_standard_df['date'])\n",
        "\n",
        "# Extracting hour from the datetime for analysis\n",
        "toast_standard_df['hour'] = toast_standard_df['date'].dt.hour\n",
        "square_standard_df['hour'] = square_standard_df['date'].dt.hour\n",
        "\n",
        "# Grouping by hour and summing up the sales for Toast data\n",
        "toast_sales_by_hour = toast_standard_df.groupby('hour')['price'].sum()\n",
        "\n",
        "# Grouping by hour and summing up the sales for Square data\n",
        "square_sales_by_hour = square_standard_df.groupby('hour')['price'].sum()\n",
        "\n",
        "toast_sales_by_hour, square_sales_by_hour\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPXqrp0zngJs"
      },
      "outputs": [],
      "source": [
        "# Grouping Toast data by service and summing up the sales\n",
        "toast_sales_by_service = toast_standard_df.groupby(toast_df['Service'])['price'].sum()\n",
        "\n",
        "toast_sales_by_service\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEKHV_gXnv2y"
      },
      "outputs": [],
      "source": [
        "# Extracting day of the week from the datetime (0=Monday, 6=Sunday)\n",
        "toast_standard_df['day_of_week'] = toast_standard_df['date'].dt.dayofweek\n",
        "square_standard_df['day_of_week'] = square_standard_df['date'].dt.dayofweek\n",
        "\n",
        "# Grouping by day of the week and summing up the sales for Toast data\n",
        "toast_sales_by_day_of_week = toast_standard_df.groupby('day_of_week')['price'].sum()\n",
        "\n",
        "# Grouping by day of the week and summing up the sales for Square data\n",
        "square_sales_by_day_of_week = square_standard_df.groupby('day_of_week')['price'].sum()\n",
        "\n",
        "toast_sales_by_day_of_week, square_sales_by_day_of_week\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38P6h4toq5xN"
      },
      "source": [
        "# **Task 4 - Top Selling Categories**\n",
        "In this task, we'll focus on analyzing the top-selling categories by:\n",
        "\n",
        "Ranking Categories by Sales Volume: Identifying which categories generate the most sales.\n",
        "Calculating Average Sale Price per Category: Finding the average price for items in each category.\n",
        "Determining the Percentage of Total Sales per Category: Assessing how much each category contributes to the overall sales.\n",
        "Focusing on Dine-In Sales Data Only: We'll filter out non-dine-in sales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VheqC6xq5Ae"
      },
      "outputs": [],
      "source": [
        "# Convert price from string to float\n",
        "square_df['Gross Sales'] = square_df['Gross Sales'].replace('[\\$,]', '', regex=True).astype(float)\n",
        "\n",
        "# Filtering Square data for Dine-In sales only\n",
        "square_dine_in_df = square_df[square_df['Dining Option'] == 'For Here']\n",
        "\n",
        "# Grouping by category for sales volume and average sale price\n",
        "category_sales_volume = square_dine_in_df.groupby('Item')['Gross Sales'].sum().sort_values(ascending=False)\n",
        "category_average_price = square_dine_in_df.groupby('Item')['Gross Sales'].mean()\n",
        "\n",
        "# Calculating the percentage of total sales per category\n",
        "total_sales = square_dine_in_df['Gross Sales'].sum()\n",
        "category_sales_percentage = (category_sales_volume / total_sales) * 100\n",
        "\n",
        "category_analysis_square = pd.DataFrame({\n",
        "    'Sales Volume': category_sales_volume,\n",
        "    'Average Price': category_average_price,\n",
        "    'Percentage of Total Sales': category_sales_percentage\n",
        "})\n",
        "\n",
        "category_analysis_square.head()  # Displaying top categories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRsp6L9tr19G"
      },
      "outputs": [],
      "source": [
        "# Convert price from string to float\n",
        "toast_df['Net Price'] = toast_df['Net Price'].replace('[\\$,]', '', regex=True).astype(float)\n",
        "\n",
        "# Filtering Toast data for Dine-In sales only\n",
        "toast_dine_in_df = toast_df[toast_df['Dining Option'] == 'Dine In']\n",
        "\n",
        "# Grouping by menu item for sales volume and average sale price\n",
        "menu_item_sales_volume = toast_dine_in_df.groupby('Menu Item')['Net Price'].sum().sort_values(ascending=False)\n",
        "menu_item_average_price = toast_dine_in_df.groupby('Menu Item')['Net Price'].mean()\n",
        "\n",
        "# Calculating the percentage of total sales per menu item\n",
        "total_sales_toast = toast_dine_in_df['Net Price'].sum()\n",
        "menu_item_sales_percentage = (menu_item_sales_volume / total_sales_toast) * 100\n",
        "\n",
        "menu_item_analysis_toast = pd.DataFrame({\n",
        "    'Sales Volume': menu_item_sales_volume,\n",
        "    'Average Price': menu_item_average_price,\n",
        "    'Percentage of Total Sales': menu_item_sales_percentage\n",
        "})\n",
        "\n",
        "menu_item_analysis_toast.head()  # Displaying top menu items\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlSvkcEJrI23"
      },
      "source": [
        "# **Task 5 - Top Selling Categories by Service**\n",
        "In this task, we'll analyze the top-selling categories grouped by service."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAnVUa9ProDX"
      },
      "outputs": [],
      "source": [
        "# Grouping Toast data by service and menu item for sales volume and average sale price\n",
        "service_category_sales_volume = toast_dine_in_df.groupby(['Service', 'Menu Item'])['Net Price'].sum().sort_values(ascending=False)\n",
        "service_category_average_price = toast_dine_in_df.groupby(['Service', 'Menu Item'])['Net Price'].mean()\n",
        "\n",
        "# Calculating the percentage of total sales per category within each service\n",
        "service_total_sales = toast_dine_in_df.groupby('Service')['Net Price'].sum()\n",
        "service_category_sales_percentage = service_category_sales_volume.div(service_total_sales, level='Service') * 100\n",
        "\n",
        "service_category_analysis_toast = pd.DataFrame({\n",
        "    'Sales Volume': service_category_sales_volume,\n",
        "    'Average Price': service_category_average_price,\n",
        "    'Percentage of Total Sales': service_category_sales_percentage\n",
        "}).reset_index()\n",
        "\n",
        "# Displaying top menu items by service\n",
        "service_category_analysis_toast.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk7U3T4ntUBi"
      },
      "source": [
        "# **Task 6 - Top Selling Dishes**\n",
        "For this task, we will identify the top 10 selling dishes by gross sales per category, focusing on dine-in sales data only. We'll calculate:\n",
        "\n",
        "The Gross Sales per Dish.\n",
        "The Percentage of Total Sales Each Dish Represents.\n",
        "The Percentage of Category Sales Each Dish Represents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FIAteTHtV5y"
      },
      "outputs": [],
      "source": [
        "# Reload the Square POS data\n",
        "square_df = pd.read_csv(file_path_square)\n",
        "square_df['Gross Sales'] = square_df['Gross Sales'].replace('[\\$,]', '', regex=True).astype(float)\n",
        "\n",
        "# Filtering for dine-in sales only\n",
        "square_dine_in_df = square_df[square_df['Dining Option'] == 'For Here']\n",
        "\n",
        "# Grouping by category and item for sales volume\n",
        "dish_sales_volume_square = square_dine_in_df.groupby(['Category', 'Item'])['Gross Sales'].sum().sort_values(ascending=False)\n",
        "\n",
        "# Calculating the percentage of total sales and category sales each dish represents\n",
        "total_sales_square = square_dine_in_df['Gross Sales'].sum()\n",
        "category_sales_square = square_dine_in_df.groupby('Category')['Gross Sales'].sum()\n",
        "dish_total_sales_percentage_square = (dish_sales_volume_square / total_sales_square) * 100\n",
        "dish_category_sales_percentage_square = dish_sales_volume_square.div(category_sales_square, level='Category') * 100\n",
        "\n",
        "# Combining the data into a single DataFrame\n",
        "top_dishes_square = pd.DataFrame({\n",
        "    'Gross Sales': dish_sales_volume_square,\n",
        "    'Percentage of Total Sales': dish_total_sales_percentage_square,\n",
        "    'Percentage of Category Sales': dish_category_sales_percentage_square\n",
        "})\n",
        "\n",
        "# Displaying the top 10 dishes\n",
        "top_dishes_square.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvetpGIKtZqC"
      },
      "outputs": [],
      "source": [
        "# Grouping by menu item for sales volume (without category) for Toast data\n",
        "dish_sales_volume_toast = toast_dine_in_df.groupby('Menu Item')['Net Price'].sum().sort_values(ascending=False)\n",
        "\n",
        "# Calculating the percentage of total sales each dish represents\n",
        "dish_total_sales_percentage_toast = (dish_sales_volume_toast / total_sales_toast) * 100\n",
        "\n",
        "# Combining the data into a single DataFrame\n",
        "top_dishes_toast_no_category = pd.DataFrame({\n",
        "    'Gross Sales': dish_sales_volume_toast,\n",
        "    'Percentage of Total Sales': dish_total_sales_percentage_toast\n",
        "})\n",
        "\n",
        "# Displaying the top 10 dishes\n",
        "top_dishes_toast_no_category.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGDB1lZdt0MK"
      },
      "source": [
        "# **Task 7 - Top Selling Dishes by Service**\n",
        "Next, we'll group the data by service and rank the top 10 dishes by greatest sales volume, again focusing on dine-in sales only. We'll calculate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKYYkvyut0mY"
      },
      "outputs": [],
      "source": [
        "# Grouping Toast data by service and menu item for sales volume\n",
        "service_dish_sales_volume_toast = toast_dine_in_df.groupby(['Service', 'Menu Item'])['Net Price'].sum().sort_values(ascending=False)\n",
        "\n",
        "# Calculating the percentage of total sales and service sales each dish represents\n",
        "service_total_sales_toast = toast_dine_in_df.groupby('Service')['Net Price'].sum()\n",
        "service_dish_total_sales_percentage_toast = service_dish_sales_volume_toast.div(service_total_sales_toast, level='Service') * 100\n",
        "\n",
        "# Combining the data into a single DataFrame\n",
        "top_dishes_by_service_toast = pd.DataFrame({\n",
        "    'Gross Sales': service_dish_sales_volume_toast,\n",
        "    'Percentage of Total Sales': service_dish_total_sales_percentage_toast\n",
        "})\n",
        "\n",
        "# Displaying the top 10 dishes by service\n",
        "top_dishes_by_service_toast.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGPyPI_tuHqI"
      },
      "source": [
        "# **Task 8 - Items Commonly Sold Together**\n",
        "This task involves identifying the top 20 pairs of items that are most commonly sold together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKuudLk_uIMA"
      },
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "from collections import Counter\n",
        "\n",
        "# Re-loading the Square POS data\n",
        "square_df = pd.read_csv(file_path_square)\n",
        "square_df['Gross Sales'] = square_df['Gross Sales'].replace('[\\$,]', '', regex=True).astype(float)\n",
        "\n",
        "# Filter out non-dine-in sales\n",
        "square_dine_in_df = square_df[square_df['Dining Option'] == 'For Here']\n",
        "\n",
        "# Grouping items by Transaction ID to find combinations\n",
        "grouped_items = square_dine_in_df.groupby('Transaction ID')['Item'].apply(list)\n",
        "\n",
        "# Generate all item pairs within each transaction, excluding duplicates and self-pairs\n",
        "item_pairs = Counter()\n",
        "for items in grouped_items:\n",
        "    for item_pair in combinations(set(items), 2):\n",
        "        # Sorting the pair to treat different orderings as the same (e.g., A-B and B-A)\n",
        "        item_pairs[tuple(sorted(item_pair))] += 1\n",
        "\n",
        "# Get the top 20 most common pairs\n",
        "top_20_pairs = item_pairs.most_common(20)\n",
        "\n",
        "# Convert to DataFrame for further analysis\n",
        "top_20_pairs_df = pd.DataFrame(top_20_pairs, columns=['Item Pair', 'Frequency'])\n",
        "\n",
        "# Calculate additional metrics\n",
        "top_20_pairs_df['Probability of Pair Sold Together'] = top_20_pairs_df['Frequency'] / len(grouped_items)\n",
        "top_20_pairs_df['Total Sales Volume'] = top_20_pairs_df['Item Pair'].apply(\n",
        "    lambda x: square_dine_in_df[\n",
        "        square_dine_in_df['Item'].isin(x)\n",
        "    ]['Gross Sales'].sum()\n",
        ")\n",
        "\n",
        "top_20_pairs_df.head()  # Display the top 5 for brevity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7T-62auyr1d"
      },
      "outputs": [],
      "source": [
        "# Reload the Toast POS data\n",
        "toast_df = pd.read_csv(file_path_toast, encoding='ISO-8859-1')\n",
        "toast_df['Net Price'] = toast_df['Net Price'].replace('[\\$,]', '', regex=True).astype(float)\n",
        "\n",
        "# Filter out non-dine-in sales\n",
        "toast_dine_in_df = toast_df[toast_df['Dining Option'] == 'Dine In']\n",
        "\n",
        "# Grouping items by Order Id to find combinations\n",
        "grouped_items_toast = toast_dine_in_df.groupby('Order Id')['Menu Item'].apply(list)\n",
        "\n",
        "# Generate all item pairs within each order, excluding duplicates and self-pairs\n",
        "item_pairs_toast = Counter()\n",
        "for items in grouped_items_toast:\n",
        "    for item_pair in combinations(set(items), 2):\n",
        "        # Sorting the pair to treat different orderings as the same\n",
        "        item_pairs_toast[tuple(sorted(item_pair))] += 1\n",
        "\n",
        "# Get the top 20 most common pairs for Toast data\n",
        "top_20_pairs_toast = item_pairs_toast.most_common(20)\n",
        "\n",
        "# Convert to DataFrame for further analysis\n",
        "top_20_pairs_toast_df = pd.DataFrame(top_20_pairs_toast, columns=['Item Pair', 'Frequency'])\n",
        "\n",
        "# Calculate additional metrics for Toast data\n",
        "top_20_pairs_toast_df['Probability of Pair Sold Together'] = top_20_pairs_toast_df['Frequency'] / len(grouped_items_toast)\n",
        "top_20_pairs_toast_df['Total Sales Volume'] = top_20_pairs_toast_df['Item Pair'].apply(\n",
        "    lambda x: toast_dine_in_df[\n",
        "        toast_dine_in_df['Menu Item'].isin(x)\n",
        "    ]['Net Price'].sum()\n",
        ")\n",
        "\n",
        "top_20_pairs_toast_df.head()  # Display the top 5 for brevity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQRCm82BzJmI"
      },
      "source": [
        "# **Task 9 - Categories Commonly Sold Together**\n",
        "This task involves identifying categories that are commonly sold together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaHIQR2TzKAd"
      },
      "outputs": [],
      "source": [
        "# Grouping items by Transaction ID to find category combinations\n",
        "grouped_categories = square_dine_in_df.groupby('Transaction ID')['Category'].apply(list)\n",
        "\n",
        "# Generate all category pairs within each transaction, excluding duplicates and self-pairs\n",
        "category_pairs = Counter()\n",
        "for categories in grouped_categories:\n",
        "    for category_pair in combinations(set(categories), 2):\n",
        "        # Sorting the pair to treat different orderings as the same\n",
        "        category_pairs[tuple(sorted(category_pair))] += 1\n",
        "\n",
        "# Get the top 20 most common category pairs\n",
        "top_category_pairs = category_pairs.most_common(20)\n",
        "\n",
        "# Convert to DataFrame for further analysis\n",
        "top_category_pairs_df = pd.DataFrame(top_category_pairs, columns=['Category Pair', 'Frequency'])\n",
        "\n",
        "# Calculate additional metrics\n",
        "top_category_pairs_df['Probability of Category Pair Sold Together'] = top_category_pairs_df['Frequency'] / len(grouped_categories)\n",
        "top_category_pairs_df['Total Sales Volume'] = top_category_pairs_df['Category Pair'].apply(\n",
        "    lambda x: square_dine_in_df[\n",
        "        square_dine_in_df['Category'].isin(x)\n",
        "    ]['Gross Sales'].sum()\n",
        ")\n",
        "\n",
        "top_category_pairs_df.head()  # Display the top 5 for brevity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fubAK1PCztmS"
      },
      "outputs": [],
      "source": [
        "# Handling null or problematic values in 'Menu Group'\n",
        "toast_dine_in_df['Menu Group'] = toast_dine_in_df['Menu Group'].fillna('Unknown').astype(str)\n",
        "grouped_menu_groups = toast_dine_in_df.groupby('Order Id')['Menu Group'].apply(list)\n",
        "\n",
        "\n",
        "# Regenerate Menu Group pairs with the corrected data\n",
        "menu_group_pairs_corrected = Counter()\n",
        "for menu_groups in grouped_menu_groups:\n",
        "    # Ensure all menu groups are strings and filter out any 'Unknown' or empty groups\n",
        "    cleaned_menu_groups = [str(group) for group in menu_groups if group and group != 'Unknown']\n",
        "    for menu_group_pair in combinations(set(cleaned_menu_groups), 2):\n",
        "        menu_group_pairs_corrected[tuple(sorted(menu_group_pair))] += 1\n",
        "\n",
        "# Get the top 20 most common Menu Group pairs with corrected data\n",
        "top_menu_group_pairs_corrected = menu_group_pairs_corrected.most_common(20)\n",
        "\n",
        "# Convert to DataFrame for further analysis\n",
        "top_menu_group_pairs_corrected_df = pd.DataFrame(top_menu_group_pairs_corrected, columns=['Menu Group Pair', 'Frequency'])\n",
        "\n",
        "# Calculate additional metrics for corrected data\n",
        "top_menu_group_pairs_corrected_df['Probability of Menu Group Pair Sold Together'] = top_menu_group_pairs_corrected_df['Frequency'] / len(grouped_menu_groups)\n",
        "top_menu_group_pairs_corrected_df['Total Sales Volume'] = top_menu_group_pairs_corrected_df['Menu Group Pair'].apply(\n",
        "    lambda x: toast_dine_in_df[\n",
        "        toast_dine_in_df['Menu Group'].isin(x)\n",
        "    ]['Net Price'].sum()\n",
        ")\n",
        "\n",
        "top_menu_group_pairs_corrected_df.head()  # Display the top 5 for brevity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwh9q0qm1TZf"
      },
      "source": [
        "# **Task 10 - Substitutes and Compliments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lds6owZq1TyX"
      },
      "outputs": [],
      "source": [
        "# Preparing the Square POS data for substitutes and complements analysis\n",
        "# We need to ensure that the data has 'item' and 'order_id' columns\n",
        "import itertools\n",
        "square_analysis_df = square_dine_in_df[['Item', 'Transaction ID']].rename(columns={'Item': 'item', 'Transaction ID': 'order_id'})\n",
        "\n",
        "# Implementing the functions from the provided script\n",
        "def get_order_count_map(df):\n",
        "    grouped_df = df.groupby(['item', 'order_id']).mean()\n",
        "    order_count_map = grouped_df.index.get_level_values(0).value_counts().to_dict()\n",
        "    return order_count_map\n",
        "\n",
        "def get_order_id_map(df):\n",
        "    unique_items = df['item'].unique()\n",
        "    order_id_map = {}\n",
        "    for item in unique_items:\n",
        "        order_id_map[item] = set(df[df['item'] == item]['order_id'].unique())\n",
        "    return order_id_map\n",
        "\n",
        "def get_substitutes_and_complements(df):\n",
        "    unique_items = df['item'].unique()\n",
        "    combinations = list(itertools.combinations(unique_items, 2))\n",
        "    combo_df = pd.DataFrame(combinations, columns=['item_x', 'item_y'])\n",
        "\n",
        "    order_count_map = get_order_count_map(df)\n",
        "    order_id_map = get_order_id_map(df)\n",
        "\n",
        "    combo_df['item_x_count'] = combo_df.apply(lambda row: order_count_map[row['item_x']], axis=1)\n",
        "    combo_df['item_y_count'] = combo_df.apply(lambda row: order_count_map[row['item_y']], axis=1)\n",
        "\n",
        "    combo_df['union_count'] = combo_df.apply(lambda row: len(order_id_map[row['item_x']].union(order_id_map[row['item_y']])), axis=1)\n",
        "    combo_df['intersection_count'] = combo_df.apply(lambda row: len(order_id_map[row['item_x']].intersection(order_id_map[row['item_y']])), axis=1)\n",
        "    combo_df['complement_ratio'] = combo_df['intersection_count'] / combo_df['union_count']\n",
        "    combo_df['min_item_count'] = combo_df.apply(lambda row: min(row['item_x_count'], row['item_y_count']), axis=1)\n",
        "    combo_df['substitute_ratio'] = combo_df['intersection_count'] / combo_df['min_item_count']\n",
        "\n",
        "    return combo_df\n",
        "\n",
        "# Running the analysis\n",
        "substitutes_and_complements_df = get_substitutes_and_complements(square_analysis_df)\n",
        "\n",
        "# Displaying a sample of the results\n",
        "substitutes_and_complements_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpDs6WCl--xq"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbhSJz99-_Gp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
